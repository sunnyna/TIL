{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/biiPKJsVkzmbW6k4ABVD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunnyna/TIL/blob/master/Model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMmO_oPhN_Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "from tensorflow import keras\n",
        "\n",
        "''' \n",
        "  You need to check the path of these.\n",
        "\n",
        "  test = pd.read_pickle(\"token_test_data.pkl\")\n",
        "  train = pd.read_pickle(\"token_train_data.pkl\")\n",
        "  ko_model= Word2Vec.load('word2vec_movie.model')\n",
        "\n",
        "  plz add these files in the right folder.\n",
        "'''\n",
        "\n",
        "# made by ChangYoon\n",
        "def plot_graphs(history, string, name='model'):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_' + string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.title(name)\n",
        "    plt.legend([string, 'val_' + string])\n",
        "    plt.show()\n",
        "    ##저장될 폴더생성\n",
        "    result_dir = './result_file'\n",
        "    if not os.path.exists(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "    plt.savefig(result_dir+'/{}.png'.format(name))\n",
        "    print('<{}.png> result_file폴더에 결과 그래프 저장 완료'.format(name))\n",
        "\n",
        "def m2_load_token_and_label():\n",
        "\n",
        "  test = pd.read_pickle(\"token_test_data.pkl\")\n",
        "  train = pd.read_pickle(\"token_train_data.pkl\")\n",
        "\n",
        "  training_sentences, training_labels = train['tokens'], train['labels']\n",
        "  testing_sentences, testing_labels = test['tokens'], test['labels']\n",
        "\n",
        "  return training_sentences, training_labels, testing_sentences, testing_labels\n",
        "\n",
        "\n",
        "def m2_tokenizer():\n",
        "\n",
        "  vocab_size = 20000\n",
        "  embedding_dim = 200\n",
        "  max_length = 30\n",
        "  truct_type = 'post'\n",
        "  padding_type = 'post'\n",
        "  oov_tok = '<OOV>'\n",
        "\n",
        "  training_sentences, training_labels, testing_sentences, testing_labels = m2_load_token_and_label()\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "  tokenizer.fit_on_texts(training_sentences)\n",
        "  word_idx = tokenizer.index_word\n",
        "\n",
        "\n",
        "  # Sequence, Padding\n",
        "  training_sequences  = tokenizer.texts_to_sequences(training_sentences)\n",
        "  training_padded = pad_sequences(training_sequences, maxlen=max_length, \n",
        "                                  padding=padding_type, truncating=truct_type)\n",
        "\n",
        "\n",
        "  testing_sequences  = tokenizer.texts_to_sequences(testing_sentences)\n",
        "  testing_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
        "                                  padding=padding_type, truncating=truct_type)\n",
        "  #word2vec weight\n",
        "  vocab_size = len(word_idx) + 1\n",
        "  embedding_dim = 200\n",
        "\n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "  ko_model= Word2Vec.load('word2vec_movie.model')\n",
        "\n",
        "  for word, idx in tokenizer.word_index.items():\n",
        "      embedding_vector = ko_model[word] if word in ko_model else None\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "  return training_padded, testing_padded, training_labels,testing_labels,embedding_matrix, vocab_size\n",
        "\n",
        "def m2_model():\n",
        "\n",
        "  embedding_dim = 200\n",
        "  filter_sizes = (3, 4, 5)\n",
        "  num_filters = 100\n",
        "  dropout = 0.5\n",
        "  hidden_dims = 100\n",
        "  max_length = 30\n",
        "\n",
        "  conv_blocks =[]\n",
        "  input_shape = (30)\n",
        "  model_input = tf.keras.layers.Input(shape=input_shape)\n",
        "  z = model_input\n",
        "\n",
        "  training_padded, testing_padded, training_labels,testing_labels,embedding_matrix, vocab_size = m2_tokenizer()\n",
        "  \n",
        "  for sz in filter_sizes:\n",
        "      embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length,\n",
        "                                          weights = [embedding_matrix], trainable = False)(z)\n",
        "      conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
        "                          kernel_size=sz,\n",
        "                          padding=\"valid\",\n",
        "                          activation=\"relu\",\n",
        "                          strides=1)(embedding)\n",
        "      conv = tf.keras.layers.GlobalAveragePooling1D()(conv)\n",
        "      conv = tf.keras.layers.Flatten()(conv)\n",
        "      conv_blocks.append(conv)\n",
        "  z = tf.keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "\n",
        "  z = tf.keras.layers.Dense(hidden_dims, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.003), bias_regularizer=tf.keras.regularizers.l2(0.003))(z)\n",
        "  z = tf.keras.layers.Dropout(dropout)(z)\n",
        "  model_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "  model = tf.keras.Model(model_input, model_output)\n",
        "\n",
        "  batch_size = 50\n",
        "  num_epochs = 10\n",
        "  min_word_count = 1\n",
        "  context = 10\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  checkpoint_dir = './ckpt2'\n",
        "  if not os.path.exists(checkpoint_dir):\n",
        "      os.makedirs(checkpoint_dir)\n",
        "  callbacks = [\n",
        "      keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=0),   \n",
        "      keras.callbacks.ModelCheckpoint(\n",
        "          filepath=checkpoint_dir + '/ckpt2-loss={loss:.3f}',\n",
        "          save_freq=500)\n",
        "      ]\n",
        "\n",
        "  history = model.fit(training_padded, training_labels, epochs=10, callbacks=callbacks, batch_size = batch_size, validation_data=(testing_padded, testing_labels))\n",
        "  accuracy_graph = plot_graphs(history, 'accuracy',name='model2_accuracy')\n",
        "  loss_graph= plot_graphs(history, 'loss',name='model2_loss')  \n",
        "\n",
        "  return model, history, accuracy_graph,loss_graph\n",
        "\n",
        "# Model2 실행 및 그래프 작성\n",
        "model, history, accuracy_graph,loss_graph = m2_model()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}